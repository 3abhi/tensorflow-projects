{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):\n",
    "    with tf.variable_scope(scope):\n",
    "        nin = x.get_shape()[1].value\n",
    "        w = tf.get_variable(\"w\", [nin, nh])\n",
    "        b = tf.get_variable(\"b\", [nh])\n",
    "        return (tf.matmul(x, w)+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "import sklearn.datasets;\n",
    "iris_ds = sklearn.datasets.load_iris(return_X_y=False)\n",
    "x = tf.placeholder(tf.float32, shape=(10, 4))\n",
    "y = tf.placeholder(tf.float32, shape=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "#Printing the dataset\n",
    "print (iris_ds[\"data\"].shape)\n",
    "print (iris_ds[\"target\"].shape)\n",
    "print (iris_ds[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(10, 10, 4) (10, 10)\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import numpy as np;\n",
    "iris_data = np.array(iris_ds[\"data\"])\n",
    "iris_label = np.array(iris_ds[\"target\"])\n",
    "length = len(iris_label)\n",
    "iris_idx = list(range(length))\n",
    "shuffle(iris_idx)\n",
    "train_idx = iris_idx[:round(length*2/3)]\n",
    "test_idx = iris_idx[round(length*2/3):]\n",
    "\n",
    "train_data,train_label = iris_data[train_idx,:],iris_label[train_idx]\n",
    "test_data,test_label = iris_data[test_idx,:],iris_label[test_idx]\n",
    "print (len(train_data))\n",
    "#Reshaping Dataset\n",
    "train_data,train_label = train_data.reshape(10,-1,4),train_label.reshape(10,-1)\n",
    "print (train_data.shape,train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(x):\n",
    "    y1 = tf.nn.relu(fc(x,nh=5,scope=\"fc1\"))\n",
    "    y2 = fc(y1,nh=3,scope = \"fc2\")\n",
    "    return tf.nn.softmax(y2)\n",
    "\n",
    "y_hat = build_model(x)\n",
    "\n",
    "entropy_loss = tf.losses.softmax_cross_entropy(logits=y_hat,onehot_labels=y)\n",
    "#For Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy for 3000 examples\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizers\n",
    "optimizer = tf.train.AdamOptimizer().minimize(entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 1.0477346\n",
      "0.4 1.0474279\n",
      "0.4 1.0474671\n",
      "0.4 1.0477847\n",
      "0.4 1.048301\n",
      "0.5 1.0489582\n",
      "0.5 1.0497135\n",
      "0.5 1.0505339\n",
      "0.5 1.0513939\n",
      "0.5 1.052273\n",
      "0.5 1.0531553\n",
      "0.5 1.0540272\n",
      "0.5 1.0548778\n",
      "0.5 1.0556984\n",
      "0.5 1.0564823\n",
      "0.5 1.057225\n",
      "0.5 1.0579218\n",
      "0.5 1.0585674\n",
      "0.5 1.0591532\n",
      "0.9 1.0596629\n",
      "0.7 1.0600713\n",
      "0.4 1.0603423\n",
      "0.5 1.0604284\n",
      "0.5 1.0602741\n",
      "0.5 1.0598235\n",
      "0.5 1.0590256\n",
      "0.5 1.0578421\n",
      "0.5 1.0562524\n",
      "0.5 1.0542531\n",
      "0.5 1.0518576\n",
      "0.5 1.0490917\n",
      "0.5 1.0459898\n",
      "0.5 1.0425904\n",
      "0.5 1.038935\n",
      "0.5 1.0350631\n",
      "0.5 1.0310137\n",
      "0.5 1.0268207\n",
      "0.5 1.0225142\n",
      "0.5 1.0181193\n",
      "0.5 1.0136557\n",
      "0.5 1.0091388\n",
      "0.5 1.0045786\n",
      "0.5 0.9999823\n",
      "0.5 0.9953535\n",
      "0.5 0.99069387\n",
      "0.5 0.98600334\n",
      "0.5 0.9812813\n",
      "0.5 0.9765272\n",
      "0.5 0.97174114\n",
      "0.5 0.9669239\n",
      "0.5 0.96207756\n",
      "0.5 0.95720524\n",
      "0.5 0.952311\n",
      "0.5 0.9474001\n",
      "0.5 0.9424782\n",
      "0.5 0.93755066\n",
      "0.5 0.9326237\n",
      "0.5 0.92770255\n",
      "0.5 0.9227921\n",
      "0.5 0.91789657\n",
      "0.6 0.91301966\n",
      "0.6 0.9081645\n",
      "0.8 0.90333337\n",
      "0.8 0.8985285\n",
      "0.8 0.89375144\n",
      "0.8 0.88900346\n",
      "0.9 0.8842856\n",
      "0.9 0.8795991\n",
      "0.9 0.8749445\n",
      "0.9 0.870323\n",
      "0.9 0.86573535\n",
      "0.9 0.86118233\n",
      "0.9 0.8566648\n",
      "1.0 0.8521835\n",
      "1.0 0.8477397\n",
      "1.0 0.84333414\n",
      "1.0 0.8389676\n",
      "1.0 0.8346413\n",
      "1.0 0.8303558\n",
      "1.0 0.8261122\n",
      "1.0 0.8219115\n",
      "1.0 0.81775427\n",
      "1.0 0.81364155\n",
      "1.0 0.80957407\n",
      "1.0 0.8055526\n",
      "1.0 0.8015526\n",
      "1.0 0.79762375\n",
      "1.0 0.7937464\n",
      "1.0 0.78991956\n",
      "1.0 0.7861429\n",
      "1.0 0.78241646\n",
      "1.0 0.7787407\n",
      "1.0 0.77511585\n",
      "1.0 0.771542\n",
      "1.0 0.7680196\n",
      "1.0 0.7645487\n",
      "1.0 0.76112956\n",
      "1.0 0.7577619\n",
      "1.0 0.754446\n",
      "1.0 0.7511818\n",
      "1.0 0.74796915\n",
      "1.0 0.7448077\n",
      "1.0 0.7416975\n",
      "1.0 0.7386381\n",
      "1.0 0.73562926\n",
      "1.0 0.7326706\n",
      "1.0 0.72976184\n",
      "1.0 0.72690266\n",
      "1.0 0.7240923\n",
      "1.0 0.7213306\n",
      "1.0 0.7186169\n",
      "1.0 0.7159508\n",
      "1.0 0.7133317\n",
      "1.0 0.71075916\n",
      "1.0 0.7082324\n",
      "1.0 0.70575094\n",
      "1.0 0.70331436\n",
      "1.0 0.70092165\n",
      "1.0 0.6985725\n",
      "1.0 0.6962661\n",
      "1.0 0.694002\n",
      "1.0 0.6917793\n",
      "1.0 0.6895977\n",
      "1.0 0.68745625\n",
      "1.0 0.6853544\n",
      "1.0 0.68329155\n",
      "1.0 0.68126696\n",
      "1.0 0.67928\n",
      "1.0 0.67733\n",
      "1.0 0.6754058\n",
      "1.0 0.67352766\n",
      "1.0 0.67168665\n",
      "1.0 0.669881\n",
      "1.0 0.66810954\n",
      "1.0 0.6663715\n",
      "1.0 0.6646661\n",
      "1.0 0.6629928\n",
      "1.0 0.66135085\n",
      "1.0 0.65973985\n",
      "1.0 0.658159\n",
      "1.0 0.6566079\n",
      "1.0 0.655086\n",
      "1.0 0.6535865\n",
      "1.0 0.65211904\n",
      "1.0 0.65068215\n",
      "1.0 0.64927304\n",
      "1.0 0.64789075\n",
      "1.0 0.64653456\n",
      "1.0 0.64520365\n",
      "1.0 0.6438911\n",
      "1.0 0.6426087\n",
      "1.0 0.6413522\n",
      "1.0 0.64011955\n",
      "1.0 0.63891023\n",
      "1.0 0.63772327\n",
      "1.0 0.6365585\n",
      "1.0 0.635415\n",
      "1.0 0.6342927\n",
      "1.0 0.63319117\n",
      "1.0 0.6321098\n",
      "1.0 0.6310482\n",
      "1.0 0.630006\n",
      "1.0 0.62898284\n",
      "1.0 0.62797827\n",
      "1.0 0.626992\n",
      "1.0 0.6260235\n",
      "1.0 0.6250674\n",
      "1.0 0.62413293\n",
      "1.0 0.6232167\n",
      "1.0 0.6223172\n",
      "1.0 0.62143385\n",
      "1.0 0.6205661\n",
      "1.0 0.61971396\n",
      "1.0 0.61887676\n",
      "1.0 0.61805433\n",
      "1.0 0.61724627\n",
      "1.0 0.6164524\n",
      "1.0 0.6156723\n",
      "1.0 0.6149058\n",
      "1.0 0.61415255\n",
      "1.0 0.6134122\n",
      "1.0 0.61268467\n",
      "1.0 0.6119695\n",
      "1.0 0.61126655\n",
      "1.0 0.6105755\n",
      "1.0 0.6098962\n",
      "1.0 0.6092283\n",
      "1.0 0.60857165\n",
      "1.0 0.60792595\n",
      "1.0 0.60729104\n",
      "1.0 0.6066666\n",
      "1.0 0.6060526\n",
      "1.0 0.6054484\n",
      "1.0 0.6048544\n",
      "1.0 0.6042699\n",
      "1.0 0.603695\n",
      "1.0 0.6031293\n",
      "1.0 0.60257274\n",
      "1.0 0.60202515\n",
      "1.0 0.6014862\n",
      "1.0 0.6009559\n",
      "1.0 0.60043395\n",
      "1.0 0.5999203\n",
      "1.0 0.59941465\n",
      "1.0 0.5989169\n",
      "1.0 0.5984269\n",
      "1.0 0.59794444\n",
      "1.0 0.59746945\n",
      "1.0 0.5970017\n",
      "1.0 0.5965411\n",
      "1.0 0.5960875\n",
      "1.0 0.59564084\n",
      "1.0 0.59520096\n",
      "1.0 0.5947676\n",
      "1.0 0.59434074\n",
      "1.0 0.5939202\n",
      "1.0 0.5935059\n",
      "1.0 0.59309775\n",
      "1.0 0.5926956\n",
      "1.0 0.5922993\n",
      "1.0 0.5919087\n",
      "1.0 0.5915238\n",
      "1.0 0.5911445\n",
      "1.0 0.59077054\n",
      "1.0 0.59040207\n",
      "1.0 0.5900387\n",
      "1.0 0.58967847\n",
      "1.0 0.58932495\n",
      "1.0 0.5889774\n",
      "1.0 0.5886346\n",
      "1.0 0.5882966\n",
      "1.0 0.5879632\n",
      "1.0 0.5876342\n",
      "1.0 0.58730763\n",
      "1.0 0.5869884\n",
      "1.0 0.5866733\n",
      "1.0 0.58636254\n",
      "1.0 0.58605593\n",
      "1.0 0.58575356\n",
      "1.0 0.5854551\n",
      "1.0 0.5851607\n",
      "1.0 0.58487016\n",
      "1.0 0.58458346\n",
      "1.0 0.58430064\n",
      "1.0 0.5840214\n",
      "1.0 0.58374363\n",
      "1.0 0.5834722\n",
      "1.0 0.5832041\n",
      "1.0 0.58293945\n",
      "1.0 0.58267826\n",
      "1.0 0.5824203\n",
      "1.0 0.5821658\n",
      "1.0 0.5819144\n",
      "1.0 0.5816663\n",
      "1.0 0.58142114\n",
      "1.0 0.58117914\n",
      "1.0 0.5809402\n",
      "1.0 0.5807042\n",
      "1.0 0.5804712\n",
      "1.0 0.580241\n",
      "1.0 0.5800136\n",
      "1.0 0.57978916\n",
      "1.0 0.5795673\n",
      "1.0 0.57934815\n",
      "1.0 0.57913166\n",
      "1.0 0.57891786\n",
      "1.0 0.57870656\n",
      "1.0 0.57849777\n",
      "1.0 0.5782914\n",
      "1.0 0.5780875\n",
      "1.0 0.57788616\n",
      "1.0 0.577687\n",
      "1.0 0.57749027\n",
      "1.0 0.5772959\n",
      "1.0 0.57710373\n",
      "1.0 0.57691365\n",
      "1.0 0.5767259\n",
      "1.0 0.57654023\n",
      "1.0 0.5763567\n",
      "1.0 0.57617533\n",
      "1.0 0.5759959\n",
      "1.0 0.57581854\n",
      "1.0 0.57564324\n",
      "1.0 0.57546985\n",
      "1.0 0.5752983\n",
      "1.0 0.57512873\n",
      "1.0 0.574961\n",
      "1.0 0.57479525\n",
      "1.0 0.5746312\n",
      "1.0 0.5744689\n",
      "1.0 0.5743085\n",
      "1.0 0.5741498\n",
      "1.0 0.57399285\n",
      "1.0 0.57383746\n",
      "1.0 0.5736837\n",
      "1.0 0.57353175\n",
      "1.0 0.57338136\n",
      "1.0 0.5732325\n",
      "1.0 0.57308525\n",
      "1.0 0.5729395\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data/np.linalg.norm(train_data)\n",
    "\n",
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Training the model\n",
    "    for epoch in range(3000):\n",
    "        for (dt,lbl) in zip(train_data,train_label):\n",
    "            #print (dt,tf.keras.utils.to_categorical(lbl,3))\n",
    "            _acc,_loss,opt= sess.run([accuracy,entropy_loss,optimizer],feed_dict={x:dt,y:tf.keras.utils.to_categorical(lbl,3)})\n",
    "        if (epoch%10==0):\n",
    "            print (_acc,_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = 'C:\\\\Users\\\\jaley\\\\git\\\\sess2'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
